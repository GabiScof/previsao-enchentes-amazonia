# -*- coding: utf-8 -*-
"""teste-modelos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mht-nA5FI8YyherzyPCWGg0sbtHCvDef
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

from sklearn.linear_model import LinearRegression, Lasso, Ridge

from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR

from scipy.stats import randint, uniform
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

"""
Detalhes no fim dos c√≥digos
Modelos escolhidos para teste:
  Lineares:
    1. Linear Regression
    2. Lasso/Ridge
  N√£o Lineares:
    1. Random Forest
    2. XGBoost
    3. MLPRegressor (Rede Neural Feedforward)
    4. SVR (com kernel RBF)
"""

df = pd.read_csv("/content/drive/MyDrive/Projeto CieÃÇncia de Dados/dados-manaus-preprocessado.csv")

X = df.drop(columns=['vazao', 'municipio', 'uf'])
y = df['vazao']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Padroniza√ß√£o para SVR e MLP
scaler_x = StandardScaler()
X_train_scaled = scaler_x.fit_transform(X_train)
X_test_scaled = scaler_x.transform(X_test)

"""# **Modelos Lineares**

* **Linear Regression**
"""

lr_model = LinearRegression()
lr_model.fit(X_train_scaled, y_train)
lr_pred = lr_model.predict(X_test_scaled)

lr_rmse = np.sqrt(mean_squared_error(y_test, lr_pred))
lr_mae = mean_absolute_error(y_test, lr_pred)
lr_r2 = r2_score(y_test, lr_pred)

print("üìè Linear Regression")
print(f"RMSE: {lr_rmse:.2f}")
print(f"MAE: {lr_mae:.2f}")
print(f"R¬≤: {lr_r2:.4f}")

plt.plot(y_test.values, label='Real')
plt.plot(lr_pred, label='Previsto (LR)')
plt.legend()
plt.title("Previs√£o de vaz√£o")
plt.show()

"""* **Lasso**"""

lasso_model = Lasso(alpha=1.0, random_state=42)
lasso_model.fit(X_train_scaled, y_train)
lasso_pred = lasso_model.predict(X_test_scaled)

lasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_pred))
lasso_mae = mean_absolute_error(y_test, lasso_pred)
lasso_r2 = r2_score(y_test, lasso_pred)

print("üß≤ Lasso Regression")
print(f"RMSE: {lasso_rmse:.2f}")
print(f"MAE: {lasso_mae:.2f}")
print(f"R¬≤: {lasso_r2:.4f}")

plt.plot(y_test.values, label='Real')
plt.plot(lasso_pred, label='Previsto (Lasso)')
plt.legend()
plt.title("Previs√£o de vaz√£o")
plt.show()

"""* **Ridge**"""

ridge_model = Ridge(alpha=1.0, random_state=42)
ridge_model.fit(X_train_scaled, y_train)
ridge_pred = ridge_model.predict(X_test_scaled)

ridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_pred))
ridge_mae = mean_absolute_error(y_test, ridge_pred)
ridge_r2 = r2_score(y_test, ridge_pred)

print("üß∞ Ridge Regression")
print(f"RMSE: {ridge_rmse:.2f}")
print(f"MAE: {ridge_mae:.2f}")
print(f"R¬≤: {ridge_r2:.4f}")

plt.plot(y_test.values, label='Real')
plt.plot(ridge_pred, label='Previsto (Ridge)')
plt.legend()
plt.title("Previs√£o de vaz√£o")
plt.show()

"""# **Modelos N√£o Lineares**

* **Random Forest**
"""

rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)

rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))
rf_mae = mean_absolute_error(y_test, rf_pred)
rf_r2 = r2_score(y_test, rf_pred)

print("üå≤ Random Forest")
print(f"RMSE: {rf_rmse:.2f}")
print(f"MAE: {rf_mae:.2f}")
print(f"R¬≤: {rf_r2:.4f}")

plt.plot(y_test.values, label='Real')
plt.plot(rf_pred, label='Previsto (RF)')
plt.legend()
plt.title("Previs√£o de vaz√£o")
plt.show()

"""* **XGBoost**"""

xgb_model = XGBRegressor(random_state=42)
xgb_model.fit(X_train, y_train)
xgb_pred = xgb_model.predict(X_test)

xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))
xgb_mae = mean_absolute_error(y_test, xgb_pred)
xgb_r2 = r2_score(y_test, xgb_pred)

print("üî∑ XGBoost")
print(f"RMSE: {xgb_rmse:.2f}")
print(f"MAE: {xgb_mae:.2f}")
print(f"R¬≤: {xgb_r2:.4f}")

plt.plot(y_test.values, label='Real')
plt.plot(xgb_pred, label='Previsto (XG)')
plt.legend()
plt.title("Previs√£o de vaz√£o")
plt.show()

"""* **MLP**"""

mlp_model = MLPRegressor(random_state=42, max_iter=1000)
mlp_model.fit(X_train_scaled, y_train)
mlp_pred = mlp_model.predict(X_test_scaled)

mlp_rmse = np.sqrt(mean_squared_error(y_test, mlp_pred))
mlp_mae = mean_absolute_error(y_test, mlp_pred)
mlp_r2 = r2_score(y_test, mlp_pred)

print("üîÅ MLP Regressor")
print(f"RMSE: {mlp_rmse:.2f}")
print(f"MAE: {mlp_mae:.2f}")
print(f"R¬≤: {mlp_r2:.4f}")

plt.plot(y_test.values, label='Real')
plt.plot(mlp_pred, label='Previsto (MLP)')
plt.legend()
plt.title("Previs√£o de vaz√£o")
plt.show()

"""* **SVR**"""

svr_model = SVR(kernel='rbf')
svr_model.fit(X_train_scaled, y_train)
svr_pred = svr_model.predict(X_test_scaled)

svr_rmse = np.sqrt(mean_squared_error(y_test, svr_pred))
svr_mae = mean_absolute_error(y_test, svr_pred)
svr_r2 = r2_score(y_test, svr_pred)

print("üìà SVR (RBF)")
print(f"RMSE: {svr_rmse:.2f}")
print(f"MAE: {svr_mae:.2f}")
print(f"R¬≤: {svr_r2:.4f}")

plt.plot(y_test.values, label='Real')
plt.plot(svr_pred, label='Previsto (SVR)')
plt.legend()
plt.title("Previs√£o de vaz√£o")
plt.show()

"""# **Compara√ß√£o**

* **Modelos Lineares**
"""

linear_results = pd.DataFrame({
    "Modelo": ["Linear", "Lasso", "Ridge"],
    "RMSE": [lr_rmse, lasso_rmse, ridge_rmse],
    "MAE": [lr_mae, lasso_mae, ridge_mae],
    "R¬≤": [lr_r2, lasso_r2, ridge_r2]
}).sort_values(by="RMSE")

plt.figure(figsize=(10, 6))
sns.barplot(data=linear_results, x="Modelo", y="RMSE")
plt.title("Desempenho dos Modelos Lineares (RMSE)")
plt.ylabel("RMSE  (menor = melhor)")
plt.xlabel("Modelo")
plt.tight_layout()
plt.show()

linear_results.reset_index(drop=True)

"""* **Modelos N√£o Lineares**"""

results_df = pd.DataFrame({
    "Modelo": ["XGBoost", "Random Forest", "MLP", "SVR (RBF)"],
    "RMSE": [xgb_rmse, rf_rmse, mlp_rmse, svr_rmse],
    "MAE": [xgb_mae, rf_mae, mlp_mae, svr_mae],
    "R¬≤": [xgb_r2, rf_r2, mlp_r2, svr_r2]
}).sort_values(by="RMSE")

plt.figure(figsize=(10, 6))
sns.barplot(data=results_df, x="Modelo", y="RMSE")
plt.title("Compara√ß√£o dos Modelos N√£o Lineares (RMSE)")
plt.ylabel("RMSE  (menor = melhor)")
plt.xlabel("Modelo")
plt.tight_layout()
plt.show()

results_df.reset_index(drop=True)

"""# **Buscando a melhor configura√ß√£o para Random Forest e XGBoost**

* **Random Forest**
"""

rf = RandomForestRegressor(random_state=42)

param_dist_rf = {
    'n_estimators': randint(100, 500),
    'max_depth': [None, 10, 20, 30, 40, 50],
    'min_samples_split': randint(2, 10),
    'min_samples_leaf': randint(1, 5),
    'max_features': [0.1, 0.5, 'sqrt', 'log2']
}

#GridSearch ficava impossivelmente demorado mesmo com poucas combina√ß√µes
random_search_rf = RandomizedSearchCV(
    rf, param_distributions=param_dist_rf,
    n_iter=30, cv=5, scoring='neg_mean_squared_error',
    n_jobs=-1, random_state=42
)

random_search_rf.fit(X_train, y_train)

best_rf = random_search_rf.best_estimator_

print("üå≥ Random Forest - Melhores hiperpar√¢metros:")
print(random_search_rf.best_params_)

"""* **XGBoost**"""

xgb = XGBRegressor(objective='reg:squarederror', random_state=42)

param_dist_xgb = {
    'n_estimators': randint(100, 500),
    'max_depth': randint(3, 10),
    'learning_rate': uniform(0.01, 0.2),
    'subsample': uniform(0.6, 0.4),
    'colsample_bytree': uniform(0.6, 0.4),
    'gamma': uniform(0, 0.5)
}

random_search_xgb = RandomizedSearchCV(
    xgb, param_distributions=param_dist_xgb,
    n_iter=30, cv=5, scoring='neg_mean_squared_error',
    n_jobs=-1, random_state=42
)

random_search_xgb.fit(X_train, y_train)

best_xgb = random_search_xgb.best_estimator_

print("‚ö° XGBoost - Melhores hiperpar√¢metros:")
print(random_search_xgb.best_params_)

"""* **Nova Compara√ß√£o**"""

rf_pred = best_rf.predict(X_test)
xgb_pred = best_xgb.predict(X_test)

rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))
xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))

rf_mae = mean_absolute_error(y_test, rf_pred)
xgb_mae = mean_absolute_error(y_test, xgb_pred)

rf_r2 = r2_score(y_test, rf_pred)
xgb_r2 = r2_score(y_test, xgb_pred)

plt.figure(figsize=(14, 5))

# Random Forest
plt.subplot(1, 2, 1)
plt.plot(y_test.values, label='Real')
plt.plot(rf_pred, label='Previsto (RF)')
plt.title(f"Random Forest\nRMSE={rf_rmse:.2f} | MAE={rf_mae:.2f} | R¬≤={rf_r2:.3f}")
plt.legend()

# XGBoost
plt.subplot(1, 2, 2)
plt.plot(y_test.values, label='Real')
plt.plot(xgb_pred, label='Previsto (XGB)')
plt.title(f"XGBoost\nRMSE={xgb_rmse:.2f} | MAE={xgb_mae:.2f} | R¬≤={xgb_r2:.3f}")
plt.legend()

plt.tight_layout()
plt.show()

# DataFrame com os resultados
results_df = pd.DataFrame({
    'Modelo': ['Random Forest', 'XGBoost'],
    'RMSE': [rf_rmse, xgb_rmse],
    'MAE': [rf_mae, xgb_mae],
    'R¬≤': [rf_r2, xgb_r2]
})

print()
plt.figure(figsize=(10, 6))
sns.barplot(data=results_df, x="Modelo", y="RMSE")
plt.title("Compara√ß√£o dos Modelos N√£o Lineares (RMSE)")
plt.ylabel("RMSE (menor = melhor)")
plt.xlabel("Modelo")
plt.tight_layout()
plt.show()

"""
Linear Regression
  Tenta encontrar uma reta (ou plano, no caso de m√∫ltiplas vari√°veis) que melhor se ajusta aos dados.
  Assumimos uma rela√ß√£o linear entre os preditores e a vari√°vel alvo.

Lasso/Ridge Regression
  S√£o regress√µes lineares com regulariza√ß√£o:
  Lasso (L1): for√ßa alguns coeficientes a zero (faz sele√ß√£o de vari√°veis).
  Ridge (L2): encolhe os coeficientes, mas n√£o zera.
  Hiperpar√¢metros principais:
    alpha:	For√ßa da regulariza√ß√£o. Maior alpha = mais penaliza√ß√£o.
    max_iter:	N√∫mero m√°ximo de itera√ß√µes (caso n√£o convirja).

Random Forest Regressor
  √â um conjunto de √°rvores de decis√£o.
  Cada √°rvore tenta prever a sa√≠da (vaz√£o), e o resultado final √© a m√©dia das previs√µes de todas as √°rvores.
  As √°rvores s√£o criadas com amostras aleat√≥rias dos dados (bootstrap).

  Hiperpar√¢metros principais:
    n_estimators:	N√∫mero de √°rvores na floresta. Ex: 100 √© um bom come√ßo.
    max_depth:	Profundidade m√°xima das √°rvores (controla complexidade).
    min_samples_split:	M√≠nimo de amostras para dividir um n√≥.
    random_state:	Garante resultados reprodut√≠veis.

XGBoost Regressor
  Modelo de boosting: treina v√°rias √°rvores pequenas sequencialmente.
  Cada nova √°rvore corrige os erros da anterior.
  Mais eficiente que Random Forest quando bem ajustado.

  Hiperpar√¢metros principais:
    n_estimators:	N√∫mero de √°rvores (itera√ß√µes).
    learning_rate:	Qu√£o r√°pido o modelo aprende (menor = mais preciso, por√©m mais lento).
    max_depth:	Profundidade m√°xima de cada √°rvore.
    subsample:	Propor√ß√£o de dados usada por √°rvore (evita overfitting).
    colsample_bytree: Propor√ß√£o de colunas usada por √°rvore.

MLP Regressor (Multi-Layer Perceptron)
  Rede neural feedforward que aprende a mapear entradas para sa√≠das por meio de camadas ocultas de neur√¥nios.
  Cada neur√¥nio aplica uma fun√ß√£o de ativa√ß√£o n√£o linear sobre uma combina√ß√£o linear dos dados de entrada.

  Usa:
    Dados normalizados (escalados com StandardScaler) para converg√™ncia mais r√°pida e est√°vel.
    Backpropagation para atualizar os pesos.
    Ativa√ß√£o padr√£o relu (fun√ß√£o linear retificada).

  Hiperpar√¢metros principais:
    hidden_layer_sizes:	Define o n√∫mero de neur√¥nios e camadas ocultas (ex: (100, 50) = duas camadas com 100 e 50 neur√¥nios).
    activation:	Fun√ß√£o de ativa√ß√£o (relu, tanh, logistic, identity).
    solver:	Otimizador usado: adam (padr√£o), lbfgs, sgd.
    alpha:	Par√¢metro de regulariza√ß√£o L2 (ajuda a evitar overfitting).
    learning_rate:	Estrat√©gia de taxa de aprendizado (constant, adaptive, etc.).
    max_iter:	N√∫mero m√°ximo de itera√ß√µes para o treinamento.
    random_state:	Para reprodutibilidade.

SVR (Support Vector Regression)
  Vers√£o para regress√£o da t√©cnica de M√°quinas de Vetores de Suporte (SVM).
  Encontra uma fun√ß√£o dentro de um tubo de toler√¢ncia Œµ em torno dos dados, penalizando os pontos fora desse intervalo.

  Usa:
    Escala dos dados √© essencial (por isso normalizamos com StandardScaler).
    Usa fun√ß√µes de kernel para transformar o espa√ßo de entrada (ex: rbf = radial basis function).

   Principais hiperpar√¢metros:
    kernel:	Fun√ß√£o de kernel (linear, poly, rbf, sigmoid).
    C:	Penalidade para erros fora da margem. Quanto maior, mais sens√≠vel a erros.
    epsilon:	Largura do intervalo onde n√£o h√° penaliza√ß√£o (margem de toler√¢ncia).
    gamma:	Influ√™ncia de um ponto de treino. Baixo = longe, alto = pr√≥ximo. Usado com rbf, poly, etc.

M√©tricas de Avalia√ß√£o:
  MAE ‚Äì Mean Absolute Error
    M√©dia dos erros absolutos.
    F√°cil de entender: erro m√©dio em unidades reais (ex: m¬≥/s).

  RMSE ‚Äì Root Mean Squared Error
    Parecido com MAE, mas erros maiores s√£o mais penalizados.
    √â a raiz quadrada do erro quadr√°tico m√©dio.

  R¬≤ ‚Äì Coeficiente de Determina√ß√£o
    Mede o quanto o modelo explica da vari√¢ncia total dos dados.
    Varia de 0 (n√£o explica nada) a 1 (explica tudo). Pode ser < 0 se o modelo for muito ruim.
"""